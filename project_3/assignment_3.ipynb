{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e37dc6-fd6a-4aeb-8cd6-b804678c7ed4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CS 7641 Machine Learning - Assignment 3\n",
    "\n",
    "student: Xinru Lu - xlu320 - xlu320@gatech.edu\n",
    "\n",
    "The repository is accessible at: https://github.com/Lorraine97/ML_2022/tree/main/project_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea46ff1-a244-4b8e-bee8-ec4046386c8a",
   "metadata": {},
   "source": [
    "## 1. Abstract\n",
    "\n",
    "This assignment will focus on 2 clustering methods in unsupervised learning, 4 dimension reduction techniques, and their performance on the two datasets we studied before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bfa9db-1379-4516-b7a1-3233bb6b3868",
   "metadata": {},
   "source": [
    "## 2. Introduction\n",
    "\n",
    "2 clustering methods to be tested and compared:\n",
    "- [k-means clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "- [Expectation Maximization](https://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture)\n",
    "\n",
    "4 dimensionality reduction algorithms:\n",
    "1. [PCA](https://scikit-learn.org/stable/modules/decomposition.html#pca)\n",
    "2. [ICA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html)\n",
    "3. [Randomized Projections](https://scikit-learn.org/stable/modules/random_projection.html)\n",
    "4. [Tree-based feature selection](https://scikit-learn.org/stable/modules/feature_selection.html#tree-based-feature-selection) (my choice of feature selection algorithm)\n",
    "\n",
    "For the above algorithms, I will first apply each of them to the two datasets, and then combine one clustering method with one dimensionality reduction algorithm (for each out of the above algorithms) to both of the datasets. Then, I will apply the 4 dimensionality reduction algorithms to one of the two datasets and train with Neural Network model. From there, I will apply the clustering algorithms first to get new data (from clustering), then train with Neural Network mode. The main purpose of the experiment is to explore and compare the performance of these methods, along with comparing with previously implemented models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38fbca2-19a9-4b93-92d9-e0e8c964b927",
   "metadata": {},
   "source": [
    "## 3. Dataset (Copied from project 1)\n",
    "\n",
    "For this project, I selected the following datasets:\n",
    "- [Check Loan Eligibility](https://www.kaggle.com/datasets/mukeshmanral/check-loan-eligibility)\n",
    "- [Student Performance (Math only)](https://www.kaggle.com/datasets/whenamancodes/student-performance)\n",
    "\n",
    "The loan eligibility data set is a set of processed data for modeling the eligibility check, based on features like gender, education, income, loan amount, credit history, etc. It consists of 12 columns, of which 10 are integer columns (binary) and 2 are decimal columns. It has a clear `y` column named `Loan_Status` (detailed information can be found in the dataset documentation). I have evaluated it from a ethical perspective in another course at OMSCS (AI Ethics) and it is an interesting data set to evaluate historical biases as well. Therefore, I would like to use it for the ML algorithms and see if different algorithms will induce different biases towards different groups.\n",
    "\n",
    "The student performance data set was drawn from student achievement in secondary education of two Portuguese schools. The data attributes include student grades, demographic, social and school related features (detailed information can be found in the dataset documentation) and it was collected by using school reports and questionnaires. I have only selected the data of the Math subject. The data was specifically modeled under binary/five-level classification and regression tasks. Interesting point about this data set is that it includes three potential `y` columns: G1, G2, G3, corresponding to 1st, 2nd and 3rd period grades. However, the G2 is dependent on G1, and G3 is dependent on both G1 and G2. I find that it could be useful for Bayesian analysis in the future. Therefore, I would like to use it for the ML course as it is versatile and easy to implement for multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d7457d-47a7-40cc-9eb8-eabaedbec683",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Method and Individual Analysis\n",
    "\n",
    "The implementation methods will be briefly introduced in the following section, but many details will be skipped since it is available on `sklearn` [documentation page](https://scikit-learn.org/stable/). For consistency, all `random_state` parameters will bet set to be 123.\n",
    "\n",
    "Evaluation methods of the following will be performed (implementation adopted from [`sklearn.metrics`](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)):\n",
    "\n",
    "1. [Silhouette Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html): determine the degree of separation between clusters\n",
    "2. [Adjusted Rand Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html): computes a similarity measure between actual labels from the dataset and the predicted labels from the clustering results\n",
    "3. [Mutual Information based scores](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html): a function that measures the agreement of the two assignments, ignoring permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d4bb4f-23cb-4b6b-b824-63ddfc36d157",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1 k-means Clustering\n",
    "\n",
    "I selected the implemented algorithm from [`sklearn.cluster.KMeans`](https://scikit-learn.org/stable/modules/clustering.html#k-means), which clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares (the distance metric here). By default, it uses [Lloyd's algorithm](https://en.wikipedia.org/wiki/Lloyd%27s_algorithm) to minimize and re-center. It would only work with numeric values, therefore pre-processing for the datasets is needed.\n",
    "\n",
    "In the experiment, I aim to find the k value to maximize the adjusted rand index and mutual information based scores when comparing the cluster results with the known labels. In this way, it would help to indicate the success of clustering on the multiple dimension data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1512730-446f-4f81-8ca8-4a8e58bc5693",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Loan Dataset \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/loan_k_means.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560e4df-0e4d-454b-bc2e-06284bc6c8d3",
   "metadata": {},
   "source": [
    "From the above chart, we observe that the fitting time and prediction time for k-means model does not change much with different k values. As k increase, the inertias of the k-means model keeps decreasing. This says that as k increase, the dataset is worse clustered by K-Means. Therefore, we should choose a smaller k to best fit the dataset. From the bottom two charts, it shows that best k value to maximize the two scores of interest will be **k = 5**. From the silhouette chart, it shows that at k = 5, it reaches a local maxima. Therefore, I think the results of the different metrics generally agree with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71a8ad-6794-420c-aac4-96e914f1273a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Math Dataset \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/math_k_means.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8cc1ce-cfed-4e5c-b309-407419fd476e",
   "metadata": {},
   "source": [
    "From the above chart, we observe that the fitting time and prediction time for k-means model does not change much with different k values for the math dataset either. As k increase, the inertias of the k-means model keeps decreasing. This says that as k increase, the dataset is worse clustered by K-Means. Therefore, we should choose a smaller k to best fit the dataset. From the bottom two charts, it shows that best k value to maximize the two scores of interest will be **k = 4**. From the silhouette chart, it shows that at k = 4, although it is not a maxima, it is still of high value in the entire chart. Therefore, I think the results of the different metrics generally agree with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f91d56-f9bb-4c67-9ca8-51a496b5f4ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 Expectation Maximization\n",
    "\n",
    "I selected the implemented algorithm from `sklearn.mixture`, which enables one to learn Gaussian Mixture Models (diagonal, spherical, tied and full covariance matrices supported), sample them, and estimate them from data. I will use the default method `covariance_type=full` so that each component has its own general covariance matrix. I plan to use `init_params=kmeans` so that it uses k-means clustering to initialize the weights, the means and the precisions. It would only work with numeric values, therefore pre-processing for the datasets is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f4e9cf-a6ba-47fb-abd5-a21d8078ce8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Loan Dataset \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/loan_gaussian.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec170d1c-ec5e-45f1-a6b9-5ade71062c58",
   "metadata": {},
   "source": [
    "From the above chart, we observe that the fitting time and prediction time for k-means model does not change much with different k values. From the bottom two charts, it shows that best n value to maximize the two scores of interest will be **n = 4**. From the silhouette chart, it shows that at n = 4, it reaches a local maxima. Therefore, I think the results of the different metrics generally agree with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166698d4-2b06-4f9e-b348-056ebb1d7423",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Math Dataset \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/math_gaussian.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e90f08-0220-4722-bddc-57efc2a8d494",
   "metadata": {},
   "source": [
    "From the above chart, we observe that the best n value to maximize the two scores of interest will be **n = 9**. From the silhouette chart, it shows that at n = 9, although it is not a maxima, it is still of high value in the entire chart. Therefore, I think the results of the different metrics generally agree with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20080f31-a5b6-4e4f-8382-f66100961800",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.3 PCA\n",
    "\n",
    "I selected the implemented algorithm from `sklearn.decomposition.PCA`. It uses linear dimensionality reduction with Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD. We will see the transformed data structure and apply the transformed data to the clustering methods.\n",
    "\n",
    "In the experiment, I grabbed the variance ratios from different dimensions of the PCA models, and achieved the following plot:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e42a9e3-2c3d-4c2d-a2a5-088fc84d668e",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/loan_pca.svg\" /><img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/math_pca.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dedc19a-b3e6-411f-9961-9deabd072eba",
   "metadata": {},
   "source": [
    "From the above chart, it is observed that for the loan dataset (on the left), with **9 features**, it can cover roughly 90% of the data variance, while for math dataset (on the right), with **30 features**, it can cover roughly 90% of the data variance. We will keep these numbers to process data for the clustering algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94593e3c-e8cd-49d1-ba49-e5d55a5897c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.4 ICA\n",
    "\n",
    "I selected the implemented algorithm from `sklearn.decomposition.FastICA`. Its purpose is to separate a multivariate signal into additive subcomponents that are maximally independent.\n",
    "\n",
    "Since there is no good metric to measure the best number of subcomponents to separate out the signals, I will adopt from the PCA results for both datasets (9 for loan, 30 for math).\n",
    "\n",
    "From the experiment, it is observed that the shape of the signals has changed after the ICA transformation. Here I attached the signal chart, colored by different columns of input data. \n",
    "\n",
    "#### Loan Dataset \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/loan_ica.svg\" />\n",
    "\n",
    "The change is not significant, but it is clear that the variance for each colored line has increased and the boundaries for the lines seem to be set free to increase variance.\n",
    "\n",
    "#### Math Dataset \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/math_ica.svg\" />\n",
    "\n",
    "The change here is clear to see: the redish band in the middle, which represent binary values, are now transformed to a changing line along with other multi-class or numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e62c5f5-2210-45d2-8e40-ba27a1b31785",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.5 Randomized Projections\n",
    "\n",
    "I selected the implemented algorithm from `sklearn.random_projection`. For the same reason as above, I choose to use the PCA results for both datasets (9 for loan, 30 for math) for the number of subcomponents.\n",
    "\n",
    "From the experiment, it is observed that the shape of the signals has changed after the Randomized Projections. Here I attached the signal chart, colored by different columns of input data. \n",
    "\n",
    "Compared to ICA, both datasets' values are more significantly different from the previous distribution. Especially for the loan dataset, the redish lines that were binary data are projected better in this model.\n",
    "\n",
    "#### Loan Dataset \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/loan_rt.svg\" />\n",
    "\n",
    "#### Math Dataset \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/math_rt.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ceadf-421c-4a3f-9431-37c68f930db1",
   "metadata": {},
   "source": [
    "### 4.6 Tree-based feature selection\n",
    "\n",
    "To implement the algorithm, I referred the documentation and used both `sklearn.ensemble.ExtraTreesClassifier` and `sklearn.feature_selection.SelectFromModel` methods. \n",
    "\n",
    "From the experiment, it is observed that the shape of the signals remained roughly the same, only layers have been removed. Here I attached the signal chart, colored by different columns of input data. \n",
    "\n",
    "Compared to all previous transformations, here the data has been filtered, instead of transformed, based on the relevance to the results. Therefore, we observe a significant color reduction (reduction of feature lines) from the original to the resulting graph, for both of the datasets. \n",
    "\n",
    "#### Loan Dataset \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/loan_tree.svg\" />\n",
    "\n",
    "#### Math Dataset \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/math_tree.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e9c8b-a876-4a17-8414-5020c492caa1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Clustering on Datasets of Reduced Dimensions\n",
    "\n",
    "In the following section, I would show the results of the clustering algorithms on the data sets that were transformed by the above 4 algorithms. In total, 16 analysis will be conducted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3219a15-8e8e-4376-9856-204b1a5e5202",
   "metadata": {},
   "source": [
    "### 5.1. k-means\n",
    "\n",
    "#### Loan Dataset\n",
    "\n",
    "From previous k-means experiment, we learn that the best k value to maximize the clustering is: k = 5. \n",
    "\n",
    "1. PCA\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/loan_pca_k.svg\" />\n",
    "\n",
    "2. ICA\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/loan_ica_k.svg\" />\n",
    "\n",
    "3. Random Projection\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/loan_rt_k.svg\" />\n",
    "\n",
    "4. Tree Selection\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/loan_tree_k.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd2c3d-5d1a-40c4-81af-e23714036590",
   "metadata": {},
   "source": [
    "From the above chart, it is observed that the optimal k values have changed:\n",
    "- PCA: k = 4\n",
    "- ICA: k = 3\n",
    "- Random Projection: k = 3\n",
    "- Tree Selection: k = 2\n",
    "\n",
    "In addition, compared to the original data and transformed data from PCA and Random Projection, it seems that ICA and Tree Selection have significantly improved the performance of the k-means model. The overall two adjusted rand index scores and mutual information scores have increased to nearly double as high, along with a much larger silhouette scores. This suggests that ICA and Tree Selection have effectively removed some noise from the original dataset that might have led to previously low performance of the k-means model. Especially with the Tree Selection algorithm, the clusters are significantly better grouped together if in the same cluster, and farther away from other groups. Overall, it seems the Tree Selection is the best transformation method here to help optimizing the k-means model for the loan dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538f9cd-0ab6-4707-a1c7-7c328e403fc8",
   "metadata": {},
   "source": [
    "#### Math Dataset\n",
    "\n",
    "From previous k-means experiment, we learn that the best k value to maximize the clustering is: k = 4.\n",
    "\n",
    "1. PCA\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/math_pca_k.svg\" />\n",
    "\n",
    "2. ICA\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/math_ica_k.svg\" />\n",
    "\n",
    "3. Random Projection\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/math_rt_k.svg\" />\n",
    "\n",
    "4. Tree Selection\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/math_tree_k.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853daafc-19c4-4c06-a670-c4305bd9477d",
   "metadata": {},
   "source": [
    "From the above chart, it is observed that the optimal k values have changed:\n",
    "\n",
    "- PCA: k = 4 (same as before)\n",
    "- ICA: k = 25\n",
    "- Random Projection: k = 10\n",
    "- Tree Selection: k = 3\n",
    "\n",
    "In addition, compared to the original data, it seems that PCA, Random Projection and Tree Selection have all slightly improved the silhouette scores. Among them, both Tree Selection and PCA have slightly improved the adjusted rand index scores and mutual information scores, while Random Projection has decreased scores. Overall, it seems that PCA and Tree Selection are good to transform the math dataset to optimize the k-means model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe4cee8-b12b-44f3-9353-ef2b453c251d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.2. Expectation Maximization\n",
    "\n",
    "#### Loan Dataset\n",
    "\n",
    "From previous EM experiment, we learn that the best n value to maximize the clustering is: n = 4. \n",
    "\n",
    "1. PCA\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/loan_pca_em.svg\" />\n",
    "\n",
    "2. ICA\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/loan_ica_em.svg\" />\n",
    "\n",
    "3. Random Projection\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/loan_rt_em.svg\" />\n",
    "\n",
    "4. Tree Selection\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/loan_tree_em.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d57704-a1b1-404e-9c5e-2269c4fbd87f",
   "metadata": {},
   "source": [
    "From the above chart, it is observed that the optimal n values have changed:\n",
    "\n",
    "- PCA: n = 6\n",
    "- ICA: n = 6\n",
    "- Random Projection: n = 5\n",
    "- Tree Selection: n = 2\n",
    "\n",
    "In addition, compared to the original data, it seems that only Tree Selection has significantly improved the silhouette score, the adjusted rand index scores and mutual information scores. Under other transformations, the performance of the EM clustering algorithm is even worse than previously. The number of clusters being 2 to best cluster the data points makes a lot of sense since the y data is a binary data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b9a8bb-ed0b-4518-9410-6169a49f74f3",
   "metadata": {},
   "source": [
    "#### Math Dataset\n",
    "From previous EM experiment, we learn that the best n value to maximize the clustering is: n = 9. \n",
    "\n",
    "1. PCA\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/math_pca_em.svg\" />\n",
    "\n",
    "2. ICA\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/math_ica_em.svg\" />\n",
    "\n",
    "3. Random Projection\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/math_rt_em.svg\" />\n",
    "\n",
    "4. Tree Selection\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Lorraine97/ML_2022/main/project_3/images/math_tree_em.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9570e2a5-8f8c-40fa-b5ed-5a91c503c62f",
   "metadata": {},
   "source": [
    "From the above chart, it is observed that the optimal n values have changed:\n",
    "\n",
    "- PCA: n = 10\n",
    "- ICA: n = 7\n",
    "- Random Projection: n = 10\n",
    "- Tree Selection: n = 10\n",
    "\n",
    "In addition, compared to the original data, it seems that PCA and Tree Selection have slightly improved the adjusted rand index scores and mutual information scores. Among them, only Tree Selection has slightly improved silhouette score. Under other transformations, the performance of the EM clustering algorithm is even worse than previously. The number of clusters being 10 to best cluster the data points makes a lot of sense since the y is a numerical data that falls into range of integers (0, 20)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82110d29-103e-4809-a0a8-151477371e5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Apply PCA-Transformed Loan Dataset to Neural Network Model\n",
    "\n",
    "Here, I selected loan dataset that has been transformed with the Tree Selection method to train and test a Neural Network model from assignment 1, and got the following results for accuracy scores:\n",
    "\n",
    "|                    | Train Accuracy | Test Accuracy |\n",
    "|--------------------|----------------|---------------|\n",
    "| Before PCA         | 0.696          | 0.714         |\n",
    "| After PCA (n = 9)  | 0.802          | 0.727         |\n",
    "| After PCA (n = 5)  | 0.765          | 0.656         |\n",
    "\n",
    "\n",
    "From the table, we observe that feature selection here seem to improved the accuracy of the Neural Network model. This is when the variance of the features cover 90% of the dataset. I got curious and decided to use a different n = 5 to further shrink the test data. Although it seems to increase the train accuracy, it lowers the test accuracy. However, another factor that might have affected the performance is the `StandardScaler()` pipeline conducted prior to the feature selection. However, it is convincing that the"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8228962-84f7-4fb5-b357-66b5f72a3f6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Neural Network with Clustered PCA-Transformed Loan Dataset\n",
    "\n",
    "Here, I selected loan dataset that has been transformed with the PCA method to go through k-means clustering first, and then using the result form the clustering as a new column in the original dataframes, to train and test a Neural Network model from assignment 1, and got the following results for accuracy scores:\n",
    "\n",
    "|                                | Train Accuracy | Test Accuracy |\n",
    "|--------------------------------|----------------|---------------|\n",
    "| Original                       | 0.696          | 0.714         |\n",
    "| PCA + k-means (k = 2, n = 9)   | 0.804          | 0.734         |\n",
    "| PCA + k-means (k = 3, n = 9)   | 0.802          | 0.727         |\n",
    "\n",
    "From the table, we observe similar behavior with part 6. However, when I got curious and decided to use a different k number (from optimal k = 3 to k = 3), I see an increase in the performance of the Neural Network model. Therefore, I think it proves that the clustering results has a significant impact on the Neural Network model's performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
